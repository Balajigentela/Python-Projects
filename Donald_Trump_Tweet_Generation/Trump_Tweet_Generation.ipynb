{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "####\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Embedding, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"Donald-Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7375 entries, 0 to 7374\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   Date                                       7375 non-null   object \n",
      " 1   Time                                       7375 non-null   object \n",
      " 2   Tweet_Text                                 7375 non-null   object \n",
      " 3   Type                                       7375 non-null   object \n",
      " 4   Media_Type                                 1225 non-null   object \n",
      " 5   Hashtags                                   2031 non-null   object \n",
      " 6   Tweet_Id                                   7375 non-null   float64\n",
      " 7   Tweet_Url                                  7375 non-null   object \n",
      " 8   twt_favourites_IS_THIS_LIKE_QUESTION_MARK  7375 non-null   int64  \n",
      " 9   Retweets                                   7375 non-null   int64  \n",
      " 10  Unnamed: 10                                26 non-null     float64\n",
      " 11  Unnamed: 11                                13 non-null     float64\n",
      "dtypes: float64(3), int64(2), object(7)\n",
      "memory usage: 691.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      Time                                         Tweet_Text  \\\n",
       "0  16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1  16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "\n",
       "   Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0  text      photo  ThankAVet  7.970000e+17   \n",
       "1  text        NaN        NaN  7.970000e+17   \n",
       "\n",
       "                                           Tweet_Url  \\\n",
       "0  https://twitter.com/realDonaldTrump/status/797...   \n",
       "1  https://twitter.com/realDonaldTrump/status/797...   \n",
       "\n",
       "   twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                     127213     41112          NaN   \n",
       "1                                     141527     28654          NaN   \n",
       "\n",
       "   Unnamed: 11  \n",
       "0          NaN  \n",
       "1          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Before...\n",
      "Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!\n",
      "...After...\n",
      "busy day planned in new york. will soon be making some very important decisions on the people who will be running our government!\n"
     ]
    }
   ],
   "source": [
    "# lowercase all\n",
    "print(\"...Before...\")\n",
    "print(df['Tweet_Text'][1])\n",
    "##\n",
    "text = df['Tweet_Text'].str.lower()\n",
    "##\n",
    "print(\"...After...\")\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Before...\n",
      "hillary advisers wanted her to avoid supporting israel when talking to democrats: https://t.co/y7m8ivu173\n",
      "...After...\n",
      "hillary advisers wanted her to avoid supporting israel when talking to democrats:\n"
     ]
    }
   ],
   "source": [
    "print(\"...Before...\")\n",
    "print(text[100])\n",
    "##\n",
    "text = text.map(lambda s: ' '.join([x for x in s.split() if 'http' not in x]))\n",
    "##\n",
    "print(\"...After...\")\n",
    "print(text[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Before...\n",
      "7375\n",
      "...After...\n",
      "6886\n"
     ]
    }
   ],
   "source": [
    "print(\"...Before...\")\n",
    "print(len(text))\n",
    "text = text[text.map(len)>40]\n",
    "print(\"...After...\")\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Before...\n",
      "join me tomorrow! minnesota ۢ 2pm michigan ۢ 6pm virginia ۢ 9:30p_\n",
      "...After...\n",
      "join me tomorrow! minnesota  2pm michigan  6pm virginia  9:30p_\n"
     ]
    }
   ],
   "source": [
    "print(\"...Before...\")\n",
    "print(text[49])\n",
    "text = text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "print(\"...After...\")\n",
    "print(text[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary:  10760\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "vocab_size = len(tokenizer.word_counts) + 1\n",
    "print(\"Total Vocabulary: \", vocab_size )\n",
    "#print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Row Count:  6886\n",
      "Training Data Count:  5508\n",
      "Test Data Coount:  1378\n"
     ]
    }
   ],
   "source": [
    "N = text.shape[0]\n",
    "print(\"Total Row Count: \", N)\n",
    "prop_train = 0.8\n",
    "train = int(N*prop_train)\n",
    "print(\"Training Data Count: \", train)\n",
    "test = N - train\n",
    "print(\"Test Data Count: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 114825\n"
     ]
    }
   ],
   "source": [
    "sequences, index_train, index_test = [], [], [] \n",
    "count = 0\n",
    "for irow,line in enumerate(text):\n",
    "    #print(irow, line)\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]    \n",
    "    #print(encoded)\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "        if irow < train:\n",
    "            index_train.append(count)\n",
    "        else:\n",
    "            index_test.append(count)\n",
    "        count += 1\n",
    "print('Total Sequences: %d' % (len(sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert padding to make each sequence of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 32\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "#print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92288, 31)\n",
      "(92288, 10760)\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "#print(y.shape)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X_train, y_train, X_test, y_test = X[index_train], y[index_train],X[index_test], y[index_test]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,\n",
    "                 input_length=1,\n",
    "                 dim_dense_embedding=10,\n",
    "                 hidden_unit_LSTM=5):\n",
    "    \n",
    "    \n",
    "    main_input = Input(shape=(input_length,),dtype='int32',name='main_input')\n",
    "    embedding = Embedding(vocab_size, dim_dense_embedding, \n",
    "                         input_length=input_length)(main_input)\n",
    "    x = LSTM(hidden_unit_LSTM)(embedding)\n",
    "    main_output = Dense(vocab_size, activation='softmax')(x)\n",
    "    model = Model(inputs=[main_input],\n",
    "                  output=[main_output])\n",
    "    #print(model.summary())\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oindrilasen/opt/anaconda3/envs/keras_env/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  del sys.path[0]\n",
      "/Users/oindrilasen/opt/anaconda3/envs/keras_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 92288 samples, validate on 22537 samples\n",
      "Epoch 1/20\n",
      " - 331s - loss: 7.0834 - accuracy: 0.0317 - val_loss: 7.0605 - val_accuracy: 0.0357\n",
      "Epoch 2/20\n",
      " - 321s - loss: 6.7485 - accuracy: 0.0417 - val_loss: 7.0808 - val_accuracy: 0.0388\n",
      "Epoch 3/20\n",
      " - 322s - loss: 6.6112 - accuracy: 0.0490 - val_loss: 6.9829 - val_accuracy: 0.0479\n",
      "Epoch 4/20\n",
      " - 323s - loss: 6.4357 - accuracy: 0.0654 - val_loss: 6.9046 - val_accuracy: 0.0691\n",
      "Epoch 5/20\n",
      " - 319s - loss: 6.2467 - accuracy: 0.0857 - val_loss: 6.8251 - val_accuracy: 0.0810\n",
      "Epoch 6/20\n",
      " - 341s - loss: 6.0597 - accuracy: 0.1031 - val_loss: 6.7698 - val_accuracy: 0.0924\n",
      "Epoch 7/20\n",
      " - 339s - loss: 5.8843 - accuracy: 0.1184 - val_loss: 6.7521 - val_accuracy: 0.0988\n",
      "Epoch 8/20\n",
      " - 356s - loss: 5.7255 - accuracy: 0.1314 - val_loss: 6.7482 - val_accuracy: 0.1012\n",
      "Epoch 9/20\n",
      " - 331s - loss: 5.5842 - accuracy: 0.1417 - val_loss: 6.7483 - val_accuracy: 0.1084\n",
      "Epoch 10/20\n",
      " - 327s - loss: 5.4563 - accuracy: 0.1505 - val_loss: 6.7600 - val_accuracy: 0.1101\n",
      "Epoch 11/20\n",
      " - 323s - loss: 5.3374 - accuracy: 0.1588 - val_loss: 6.7743 - val_accuracy: 0.1064\n",
      "Epoch 12/20\n",
      " - 317s - loss: 5.2248 - accuracy: 0.1659 - val_loss: 6.7904 - val_accuracy: 0.1080\n",
      "Epoch 13/20\n",
      " - 317s - loss: 5.1184 - accuracy: 0.1724 - val_loss: 6.8168 - val_accuracy: 0.1112\n",
      "Epoch 14/20\n",
      " - 317s - loss: 5.0175 - accuracy: 0.1786 - val_loss: 6.8363 - val_accuracy: 0.1120\n",
      "Epoch 15/20\n",
      " - 321s - loss: 4.9215 - accuracy: 0.1840 - val_loss: 6.8672 - val_accuracy: 0.1138\n",
      "Epoch 16/20\n",
      " - 334s - loss: 4.8309 - accuracy: 0.1897 - val_loss: 6.9133 - val_accuracy: 0.1146\n",
      "Epoch 17/20\n",
      " - 312s - loss: 4.7432 - accuracy: 0.1946 - val_loss: 6.9512 - val_accuracy: 0.1163\n",
      "Epoch 18/20\n",
      " - 313s - loss: 4.6583 - accuracy: 0.2008 - val_loss: 6.9940 - val_accuracy: 0.1155\n",
      "Epoch 19/20\n",
      " - 311s - loss: 4.5795 - accuracy: 0.2068 - val_loss: 7.0494 - val_accuracy: 0.1170\n",
      "Epoch 20/20\n",
      " - 315s - loss: 4.5006 - accuracy: 0.2121 - val_loss: 7.0764 - val_accuracy: 0.1179\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size,\n",
    "                               input_length=X.shape[1],\n",
    "                               dim_dense_embedding=30,\n",
    "                               hidden_unit_LSTM=64)\n",
    "\n",
    "# compile network\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "tf_model = model.fit(X_train, y_train, \n",
    "                 validation_data = (X_test,y_test),\n",
    "                 epochs=20, verbose=2,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/Users/oindrilasen/WORK_AREA/Data Science/Projects/Trump_Tweet_Generation/models/trump_tweets_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/Users/oindrilasen/WORK_AREA/Data Science/Projects/Trump_Tweet_Generation/models/trump_tweets_generator_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America was made on the rails yesterday to be tough tonight in math realdonaldtrump while time trump is trumptrain we must make up so horrible talk about them the country paulteutulsr cost of they get this lead for the boy is a success door and anncoulter needed hillary nice words by the s 4 made to vote all in order to make america great again live to have seen your volunteers thanks will make our win country i called it ohio again are unless we unearthed it again got experts up and end but i am far at congress that the\n"
     ]
    }
   ],
   "source": [
    "in_text = \"America\"\n",
    "for _ in range(100):\n",
    "    # encode the text as integer\n",
    "    enc = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    #print(enc)\n",
    "    # pre-pad sequences to a fixed length\n",
    "    enc_padding = pad_sequences([enc], maxlen=max_length-1, padding='pre')\n",
    "    #print(enc_padding)\n",
    "    probs = model.predict(enc_padding, verbose=0).flatten()\n",
    "    #print(probs)\n",
    "    index = np.random.choice(range(len(probs)),p=probs\n",
    "    #print(index)\n",
    "    index_word = {v: k for k,v in tokenizer.word_index.items()}\n",
    "    word = index_word[index] \n",
    "    in_text += ' ' + word\n",
    "print(in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
